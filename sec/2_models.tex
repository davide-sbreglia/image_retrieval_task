\section{Models Considered}
\label{sec:models}
\subsectionn{Litterature References}

ResNet-50 [He et al., 2016]: Introduces deep residual learning via identity skip connections, enabling the training of very deep convolutional networks.

EfficientNet-B0 [Tan and Le, 2019]: Scales depth, width, and resolution uniformly using a compound coefficient, providing an optimal balance between accuracy and efficiency.

Vision Transformer (ViT-B/16) [Dosovitskiy et al., 2020]: Applies transformer architectures directly to image patches, challenging CNN dominance in visual recognition tasks.

Feature Aggregation and Embedding Fusion [Zhou et al., 2021]: Justifies ensemble averaging of embeddings from heterogeneous backbones to improve generalization in retrieval.

\subsection{Theoretical Model Descriptions}
ResNet-50
ResNet-50 is a convolutional neural network comprising 50 layers with residual connections that mitigate vanishing gradient problems. In our pipeline, we remove the final classification layer and use the feature maps from the penultimate layer, which are then projected into a shared 512-dimensional space and L2-normalized for retrieval.

EfficientNet-B0
EfficientNet-B0 applies neural architecture search and compound scaling to achieve superior performance with fewer parameters. The extracted feature embeddings (1280-dim) from the features block are pooled and projected to 512-dim vectors using a projection head, followed by normalization.

Vision Transformer (ViT-B/16)
The ViT-B/16 model splits input images into fixed-size patches and processes them via standard transformer encoders. After discarding the classification head, we use the output of the [CLS] token for image representation, further projected and normalized. Its ability to capture global context complements the local focus of CNNs.

Ensemble Strategies
We construct two ensembles: (i) ResNet-50 + ViT-B/16 and (ii) EfficientNet-B0 + ViT-B/16. In both cases, normalized embeddings from the respective models are averaged before similarity computation. This leverages the orthogonal strengths of each backbone, combining spatial locality from CNNs and global context from transformers.

Training Strategy
Each model was fine-tuned only at the classification head level to prevent overfitting and reduce training time. The training loop optimizes cross-entropy loss using Adam, with early stopping and model checkpointing based on validation loss. After fine-tuning, the backbone is frozen, and the feature extractor is reused for retrieval.

