\subsection{Validation Protocol}

To evaluate our model–selection and training strategy we created an
identity-stratified validation split from the original training set.  
The dataset contains \textbf{921} identities, each with \textbf{5} labelled
images, for a total of 4\,605 pictures.  
We assigned \textbf{80\,\%} of the identities (737 people) to the training
subset and the remaining \textbf{20\,\%} (184 people) to the validation
subset; therefore no identity appears in both sets.  
This yields 920 validation images.

To mimic the competition set-up, we further split the validation images of
each identity into a \textit{gallery} and a \textit{query} part:  
one image is chosen at random for the gallery, while the other four act as
queries.  The validation benchmark therefore consists of 184 gallery
embeddings and 736 query embeddings.

We report three metrics that match the official leaderboard scoring:

\begin{itemize}
  \item \textbf{Top-1 Accuracy} – the correct identity is ranked first;
  \item \textbf{Top-5 Accuracy} – the correct identity appears within the first five results;
  \item \textbf{Top-10 Accuracy} – the correct identity appears within the first ten results.
\end{itemize}

Cosine similarity is computed on \mbox{L$_2$-normalised} embeddings.  
In line with the two-hour competition limit, every backbone is fine-tuned
for \textbf{3} epochs with a batch size of \textbf{32} and a learning rate of
\textbf{0.1} using the Adam optimiser.  
All experiments run on an Azure NC6 v3 instance equipped with a single
NVIDIA V100 GPU (16 GB).

\noindent\textit{Implementation details.}  
The split is generated by \texttt{main.py split} (listing 1); the
identity-stratified benchmark is performed in
\texttt{choose\_best\_model()} (Alg.~2), and metric computation relies on
\texttt{evaluate\_topk()}.
